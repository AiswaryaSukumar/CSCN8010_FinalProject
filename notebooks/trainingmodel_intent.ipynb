{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1ec5c7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03d3297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aishw\\OneDrive\\Desktop\\AIML\\FoundationOfML\\FinalProject\\CSCN8010_FinalProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b19a05",
   "metadata": {},
   "source": [
    "### Load SentenceTransformer\n",
    "\n",
    "In this cell, we load a pre-trained SentenceTransformer model (all-mpnet-base-v2).\n",
    "This model converts each text query (a sentence) into a 384-dimensional numerical vector called an embedding.\n",
    "\n",
    "Why do we need embeddings?\n",
    "\n",
    "Machine learning models cannot understand raw text.\n",
    "So we convert text ‚Üí numbers.\n",
    "\n",
    "Example:\n",
    "\"How do I pay my fees?\" ‚Üí [0.12, -0.04, 0.88, ...] (384 numbers)\n",
    "\n",
    "These vectors capture meaning, so similar sentences have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7b70cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aishw\\OneDrive\\Desktop\\AIML\\FoundationOfML\\FinalProject\\CSCN8010_FinalProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aishw\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Load a sentence embedding model (384-dim)\n",
    "# This is similar to what your prof described (embeddings with HF backbone)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def encode_texts(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Encode a list of texts to a 2D numpy array of embeddings.\n",
    "    shape = (n_samples, 384)\n",
    "    \"\"\"\n",
    "    embeddings = embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=False\n",
    "    )\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa5582",
   "metadata": {},
   "source": [
    "### Load your CSV and prepare data\n",
    "\n",
    "In this step, we load our labeled intent dataset from a CSV file containing two columns: text and label.\n",
    "We extract the text questions and their corresponding intent labels into Python lists so they can be used for training.\n",
    "The text list will later be converted into numerical embeddings (vectors) using the SentenceTransformer model.\n",
    "The label list will be encoded into numbers so the neural network can learn from them.\n",
    "This step simply prepares our raw dataset so it can be processed in later training cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390f8338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>small_talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>small_talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey there</td>\n",
       "      <td>small_talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good morning</td>\n",
       "      <td>small_talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good evening</td>\n",
       "      <td>small_talk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text       label\n",
       "0            hi  small_talk\n",
       "1         hello  small_talk\n",
       "2     hey there  small_talk\n",
       "3  good morning  small_talk\n",
       "4  good evening  small_talk"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 ‚Äî Load CSV and prepare data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your intent dataset\n",
    "df = pd.read_csv('../data/raw/training_data.csv')   \n",
    "\n",
    "# Extract the text and labels as lists\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3836f",
   "metadata": {},
   "source": [
    "### Generate Text Embeddings\n",
    "\n",
    "In this step, we convert all text queries into numerical embeddings using a SentenceTransformer model.\n",
    "Embeddings are dense vector representations (384-dimensional) that capture the meaning of each sentence.\n",
    "These vectors become the input features for our deep learning classifier.\n",
    "Without embeddings, the neural network cannot understand text, so this step converts language into numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc327b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 18.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (125, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 ‚Äî Convert texts into embeddings using SentenceTransformer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a lightweight embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def encode_texts(text_list):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into numerical embeddings.\n",
    "    Returns a NumPy array of shape (num_samples, 384)\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(text_list, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Convert all text queries into embeddings\n",
    "X = encode_texts(texts)\n",
    "\n",
    "print(\"Embedding shape:\", X.shape)   # Example output: (100, 384)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fab7a",
   "metadata": {},
   "source": [
    "### Encode Labels\n",
    "\n",
    "In this step, we convert the text labels (like ‚Äúfaq‚Äù, ‚Äúresource‚Äù, ‚Äúofframp‚Äù, ‚Äúchitchat‚Äù) into numbers because neural networks can only work with numeric data.\n",
    "We use LabelEncoder to map each label to a unique integer (for example: faq ‚Üí 0, resource ‚Üí 1).\n",
    "Then we convert these numeric labels into PyTorch tensors so they can be used during training.\n",
    "This ensures that both the input embeddings and the labels are in a format the deep learning model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3f7854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {np.str_('out_of_scope'): 0, np.str_('serious_issue'): 1, np.str_('small_talk'): 2, np.str_('student_affairs'): 3}\n",
      "Example encoded labels: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äî Encode labels into integer IDs\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "# Initialize label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Convert labels (strings) into numbers\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Convert to torch tensor\n",
    "y = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Show mapping\n",
    "label_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "print(\"Example encoded labels:\", y[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bc002",
   "metadata": {},
   "source": [
    "### Train/Val/Test Split + PyTorch Datasets\n",
    "In this step, we split our dataset into three parts: training, validation, and test sets.\n",
    "The training set is used by the model to learn patterns, the validation set helps tune the model and avoid overfitting, and the test set is used at the end to evaluate real performance.\n",
    "We first create a temporary split between training+validation and test, and then split the training part again into train and validation.\n",
    "This ensures the model never sees the test data while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3b9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 89\n",
      "Validation size: 23\n",
      "Test size: 13\n",
      "DataLoaders ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 ‚Äî Create Train/Val/Test split and PyTorch datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Train / Validation / Test Split\n",
    "# ------------------------------\n",
    "\n",
    "# First: split into 90% training+validation and 10% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42\n",
    ")\n",
    "\n",
    "# Second: split training data into 80% train and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n",
    "\n",
    "# ------------------------------\n",
    "# 2. PyTorch Dataset Wrapper\n",
    "# ------------------------------\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = QueryDataset(X_train, y_train)\n",
    "val_dataset = QueryDataset(X_val, y_val)\n",
    "test_dataset = QueryDataset(X_test, y_test)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. DataLoaders\n",
    "# ------------------------------\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "print(\"DataLoaders ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305864ef",
   "metadata": {},
   "source": [
    "### Build the Deep Learning Model (MLP Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972eb810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 384\n",
      "Number of classes: 4\n",
      "Classes: ['out_of_scope' 'serious_issue' 'small_talk' 'student_affairs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntentClassifier(\n",
       "  (fc1): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 ‚Äî Define the Deep Learning Model (MLP Classifier)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Number of features per embedding (for all-MiniLM-L6-v2 it's 384)\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "# Number of output classes (intents)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Input dim:\", input_dim)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "\n",
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, output_dim=4, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Dropout to reduce overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # Output layer: one neuron per class\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        x = F.relu(self.fc1(x))   # non-linear activation\n",
    "        x = self.dropout(x)       # apply dropout during training\n",
    "        logits = self.fc2(x)      # raw scores for each class\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate the model with the correct sizes\n",
    "model = IntentClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=128,\n",
    "    output_dim=num_classes,\n",
    "    dropout_p=0.3\n",
    ")\n",
    "\n",
    "# Move model to device (CPU or GPU)\n",
    "model = model.to(device)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54b7c8",
   "metadata": {},
   "source": [
    "### Training the Deep Learning Model\n",
    "\n",
    "In this step, we train the intent classifier using the training data and monitor its performance on the validation set.\n",
    "For each epoch, the model updates its weights to minimize the cross-entropy loss between predicted and true labels.\n",
    "We also calculate validation loss and accuracy to see if the model is improving or starting to overfit.\n",
    "Early stopping is used: if the validation loss does not improve for a few epochs, training stops to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d1c96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.0429 | Val Loss: 0.1997 | Val Acc: 91.30%\n",
      "  ‚úÖ New best model saved.\n",
      "Epoch 2/20 - Train Loss: 0.0285 | Val Loss: 0.1814 | Val Acc: 91.30%\n",
      "  ‚úÖ New best model saved.\n",
      "Epoch 3/20 - Train Loss: 0.0182 | Val Loss: 0.1754 | Val Acc: 91.30%\n",
      "  ‚úÖ New best model saved.\n",
      "Epoch 4/20 - Train Loss: 0.0165 | Val Loss: 0.1843 | Val Acc: 91.30%\n",
      "Epoch 5/20 - Train Loss: 0.0140 | Val Loss: 0.1830 | Val Acc: 91.30%\n",
      "Epoch 6/20 - Train Loss: 0.0092 | Val Loss: 0.1760 | Val Acc: 91.30%\n",
      "‚èπÔ∏è Early stopping at epoch 6. Best Val Loss: 0.1754\n",
      "\n",
      "Training finished.\n",
      "Best model saved at: ../models\\intent_classifier_best.pt\n",
      "Label encoder saved at: ../models\\intent_label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 ‚Äî Train the model with early stopping and save the best version\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "\n",
    "# Directory OUTSIDE the notebook\n",
    "MODEL_DIR = \"../models\"   \n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"intent_classifier_best.pt\")\n",
    "ENCODER_PATH = os.path.join(MODEL_DIR, \"intent_label_encoder.pkl\")\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # -----------------------\n",
    "    # Training\n",
    "    # -----------------------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # -----------------------\n",
    "    # Validation\n",
    "    # -----------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "        f\"- Train Loss: {avg_train_loss:.4f} \"\n",
    "        f\"| Val Loss: {avg_val_loss:.4f} \"\n",
    "        f\"| Val Acc: {val_accuracy:.2%}\"\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Early stopping\n",
    "    # -----------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "        # Save encoder\n",
    "        with open(ENCODER_PATH, \"wb\") as f:\n",
    "            pickle.dump(le, f)\n",
    "\n",
    "        print(\"  ‚úÖ New best model saved.\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(\"Best model saved at:\", MODEL_PATH)\n",
    "print(\"Label encoder saved at:\", ENCODER_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f5087",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60894e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Accuracy: 100.00% | Test Loss: 0.0711\n"
     ]
    }
   ],
   "source": [
    "# Load best trained model\n",
    "MODEL_PATH = \"../models/intent_classifier_best.pt\"\n",
    "ENCODER_PATH = \"../models/intent_label_encoder.pkl\"\n",
    "\n",
    "# Recreate model\n",
    "num_classes = len(le.classes_)\n",
    "model = IntentClassifier(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=128,\n",
    "    output_dim=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Load best weights\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"\\nüìä Test Accuracy: {test_accuracy:.2%} | Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef0074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 70.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm feeling anxious about my exams. ‚Üí student_affairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 153.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do I submit my OSAP documents? ‚Üí student_affairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 119.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! how are you? ‚Üí small_talk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 148.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to book an appointment with a student advisor. ‚Üí student_affairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 151.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is working on the portal, I feel stressed. ‚Üí serious_issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 137.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I register for courses? ‚Üí student_affairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 135.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you guide me through career services? ‚Üí student_affairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_query(text):\n",
    "    # 1. Convert text ‚Üí embedding\n",
    "    embedding = encode_texts([text])  # shape (1, 384)\n",
    "    tensor = torch.tensor(embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 2. Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        pred_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # 3. Convert index ‚Üí label\n",
    "    return le.inverse_transform([pred_idx])[0]\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"I'm feeling anxious about my exams.\",\n",
    "    \"Where do I submit my OSAP documents?\",\n",
    "    \"Hey! how are you?\",\n",
    "    \"I want to book an appointment with a student advisor.\",\n",
    "    \"Nothing is working on the portal, I feel stressed.\",\n",
    "    \"How do I register for courses?\",\n",
    "    \"Can you guide me through career services?\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    print(f\"{text} ‚Üí {classify_query(text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
